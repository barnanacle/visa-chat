/**
 * PDF ê¸°ë°˜ ë²¡í„° DB ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸
 * source_data/ ë””ë ‰í† ë¦¬ì˜ PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  JSONìœ¼ë¡œ ì €ì¥
 * ì„œë²„ì—ì„œëŠ” ì´ JSONì„ ë¡œë“œí•˜ì—¬ ChromaDBì— ì‚½ì…
 */

import dotenv from 'dotenv';
dotenv.config();

import fs from 'fs/promises';
import path from 'path';
import crypto from 'crypto';
import { fileURLToPath } from 'url';
import { createRequire } from 'module';
import { OpenAIEmbeddings } from '@langchain/openai';

// CommonJS require for pdf-parse (ESM compatibility)
const require = createRequire(import.meta.url);
const { PDFParse } = require('pdf-parse');

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const ROOT_DIR = path.join(__dirname, '..');

const SOURCE_DIR = path.join(ROOT_DIR, 'source_data');
const HASH_FILE = path.join(ROOT_DIR, '.pdf-hash');
const OUTPUT_FILE = path.join(ROOT_DIR, 'vectordb_data.json');

// OpenAI ì„ë² ë”©
const embeddings = new OpenAIEmbeddings({
    openAIApiKey: process.env.OPENAI_API_KEY
});

/**
 * PDF íŒŒì¼ í•´ì‹œ ê³„ì‚°
 */
async function calculateHashes() {
    try {
        const files = await fs.readdir(SOURCE_DIR);
        const pdfFiles = files.filter(f => f.toLowerCase().endsWith('.pdf'));

        const hashes = {};
        for (const file of pdfFiles) {
            const filePath = path.join(SOURCE_DIR, file);
            const content = await fs.readFile(filePath);
            hashes[file] = crypto.createHash('md5').update(content).digest('hex');
        }

        return hashes;
    } catch (error) {
        console.error('source_data ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
        return {};
    }
}

/**
 * ì €ì¥ëœ í•´ì‹œ ë¡œë“œ
 */
async function loadStoredHashes() {
    try {
        const content = await fs.readFile(HASH_FILE, 'utf8');
        return JSON.parse(content);
    } catch {
        return {};
    }
}

/**
 * í•´ì‹œ ì €ì¥
 */
async function saveHashes(hashes) {
    await fs.writeFile(HASH_FILE, JSON.stringify(hashes, null, 2));
}

/**
 * PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
 */
async function extractTextFromPDF(filePath) {
    console.log(`  ğŸ“„ PDF ì½ëŠ” ì¤‘: ${path.basename(filePath)}`);
    const dataBuffer = await fs.readFile(filePath);
    // PDFParse v2 requires Uint8Array
    const uint8Array = new Uint8Array(dataBuffer);
    const parser = new PDFParse(uint8Array);
    await parser.load();
    const result = await parser.getText();

    // getText() returns {pages: [{text, num}, ...]}
    if (result && result.pages && Array.isArray(result.pages)) {
        const textParts = result.pages.map(page => page.text || '');
        return textParts.join('\n\n');
    }

    return '';
}

/**
 * í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (í–¥ìƒëœ ë²„ì „)
 */
function preprocessText(text) {
    // ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    text = text.replace(/\s+/g, ' ').trim();

    // ìˆ«ì ì •ë³´ íƒœê¹…
    text = text.replace(/(\d+(?:,\d+)*(?:\.\d+)?)/g, '<num>$1</num>');

    // ë‚ ì§œ ì •ë³´ íƒœê¹…
    text = text.replace(/(\d{4}[\.\/\-]?\d{1,2}[\.\/\-]?\d{1,2})/g, '<date>$1</date>');

    // ê¸°ê°„ íƒœê¹…
    text = text.replace(/(\d+)\s*(ì¼|ê°œì›”|ë…„|ì£¼)/gi, '<duration>$1$2</duration>');

    // ë¹„ì ìœ í˜• íƒœê¹…
    text = text.replace(/([\w\d\-]+\s*ë¹„ì)/gi, '<visa>$1</visa>');

    return text;
}

/**
 * í…ìŠ¤íŠ¸ ì²­í‚¹ (ì˜ë¯¸ ê¸°ë°˜)
 */
function chunkText(text, source) {
    const chunks = [];

    // í˜ì´ì§€/ì„¹ì…˜ ê¸°ë°˜ ë¶„í•  (PDF íŠ¹ì„±ìƒ í¼í”¼ë“œ, ì—¬ëŸ¬ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬)
    const sections = text.split(/\n{3,}|\f/);

    let chunkId = 0;
    for (const section of sections) {
        const cleanedSection = section.trim();

        // ë„ˆë¬´ ì§§ì€ ì„¹ì…˜ ê±´ë„ˆë›°ê¸°
        if (cleanedSection.length < 100) continue;

        // í° ì„¹ì…˜ì€ ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
        if (cleanedSection.length > 2000) {
            const paragraphs = cleanedSection.split(/\n{2,}/);
            let currentChunk = '';

            for (const para of paragraphs) {
                if ((currentChunk + para).length > 1500) {
                    if (currentChunk.trim().length > 100) {
                        chunks.push({
                            id: `${source.replace(/[^a-zA-Z0-9]/g, '_')}_${chunkId++}`,
                            content: currentChunk.trim(),
                            metadata: {
                                source,
                                chunkIndex: chunkId,
                                contentLength: currentChunk.trim().length
                            }
                        });
                    }
                    currentChunk = para;
                } else {
                    currentChunk += '\n\n' + para;
                }
            }

            if (currentChunk.trim().length > 100) {
                chunks.push({
                    id: `${source.replace(/[^a-zA-Z0-9]/g, '_')}_${chunkId++}`,
                    content: currentChunk.trim(),
                    metadata: {
                        source,
                        chunkIndex: chunkId,
                        contentLength: currentChunk.trim().length
                    }
                });
            }
        } else {
            chunks.push({
                id: `${source.replace(/[^a-zA-Z0-9]/g, '_')}_${chunkId++}`,
                content: cleanedSection,
                metadata: {
                    source,
                    chunkIndex: chunkId,
                    contentLength: cleanedSection.length
                }
            });
        }
    }

    return chunks;
}

/**
 * ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
 */
async function generateEmbeddings(texts) {
    const MAX_CHARS = 6000; // í† í° ì œí•œ ê³ ë ¤
    const batchSize = 5;
    let allEmbeddings = [];

    // í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ê¸¸ì´ ì œí•œ
    const processedTexts = texts.map(text => {
        const processed = preprocessText(text);
        return processed.length > MAX_CHARS ? processed.substring(0, MAX_CHARS) : processed;
    });

    for (let i = 0; i < processedTexts.length; i += batchSize) {
        const batch = processedTexts.slice(i, i + batchSize);
        const progress = Math.round((i / processedTexts.length) * 100);
        console.log(`  ğŸ“Š ì„ë² ë”© ìƒì„± ì¤‘... ${progress}% (${i}/${processedTexts.length})`);

        try {
            const batchEmbeddings = await embeddings.embedDocuments(batch);
            allEmbeddings = [...allEmbeddings, ...batchEmbeddings];

            // API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
            await new Promise(resolve => setTimeout(resolve, 200));
        } catch (error) {
            console.error(`  âŒ ë°°ì¹˜ ì˜¤ë¥˜: ${error.message}`);

            // ê°œë³„ ì²˜ë¦¬ ì‹œë„
            for (const text of batch) {
                try {
                    const single = await embeddings.embedDocuments([text.substring(0, 3000)]);
                    allEmbeddings.push(single[0]);
                } catch {
                    console.error(`  âš ï¸ ì„ë² ë”© ì‹¤íŒ¨, ë¹ˆ ë²¡í„° ì‚¬ìš©`);
                    allEmbeddings.push(new Array(1536).fill(0));
                }
            }
        }
    }

    return allEmbeddings;
}

/**
 * ë©”ì¸ ë¹Œë“œ í•¨ìˆ˜
 */
async function buildVectorDB() {
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    console.log('ğŸ“š PDF ê¸°ë°˜ ë²¡í„° DB ë¹Œë“œ ì‹œì‘');
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

    // 1. í•´ì‹œ ë¹„êµë¡œ ë³€ê²½ í™•ì¸
    console.log('1ï¸âƒ£  íŒŒì¼ ë³€ê²½ í™•ì¸...');
    const currentHashes = await calculateHashes();
    const storedHashes = await loadStoredHashes();

    const pdfFiles = Object.keys(currentHashes);
    if (pdfFiles.length === 0) {
        console.log('   âŒ source_data/ ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.');
        return;
    }
    console.log(`   ğŸ“ ë°œê²¬ëœ PDF: ${pdfFiles.length}ê°œ`);

    // ë³€ê²½ ê°ì§€
    let hasChanges = false;
    for (const file of pdfFiles) {
        if (currentHashes[file] !== storedHashes[file]) {
            console.log(`   ğŸ“ ë³€ê²½ë¨: ${file}`);
            hasChanges = true;
        }
    }

    for (const file of Object.keys(storedHashes)) {
        if (!currentHashes[file]) {
            console.log(`   ğŸ—‘ï¸ ì‚­ì œë¨: ${file}`);
            hasChanges = true;
        }
    }

    // ê¸°ì¡´ ë°ì´í„° íŒŒì¼ë„ í™•ì¸
    let existingData = false;
    try {
        await fs.access(OUTPUT_FILE);
        existingData = true;
    } catch { }

    if (!hasChanges && existingData) {
        console.log('   âœ… ë³€ê²½ ì—†ìŒ. ê¸°ì¡´ ë²¡í„° ë°ì´í„° ì‚¬ìš©.\n');
        return;
    }

    // 2. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹
    console.log('\n2ï¸âƒ£  PDF ì²˜ë¦¬ ì¤‘...');
    const allChunks = [];

    for (const file of pdfFiles) {
        console.log(`\n   ğŸ“– ì²˜ë¦¬ ì¤‘: ${file}`);
        const filePath = path.join(SOURCE_DIR, file);

        try {
            const text = await extractTextFromPDF(filePath);
            console.log(`      ì¶”ì¶œëœ ë¬¸ì: ${text.length.toLocaleString()}ì`);

            const chunks = chunkText(text, file);
            console.log(`      ìƒì„±ëœ ì²­í¬: ${chunks.length}ê°œ`);
            allChunks.push(...chunks);
        } catch (error) {
            console.error(`      âŒ ì˜¤ë¥˜: ${error.message}`);
        }
    }

    console.log(`\n   ğŸ“Š ì´ ì²­í¬ ìˆ˜: ${allChunks.length}ê°œ`);

    if (allChunks.length === 0) {
        console.log('   âŒ ì²˜ë¦¬í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.');
        return;
    }

    // 3. ì„ë² ë”© ìƒì„±
    console.log('\n3ï¸âƒ£  ì„ë² ë”© ìƒì„± ì¤‘...');
    const texts = allChunks.map(c => c.content);
    const embeddingVectors = await generateEmbeddings(texts);

    // 4. JSONìœ¼ë¡œ ì €ì¥
    console.log('\n4ï¸âƒ£  ë°ì´í„° ì €ì¥ ì¤‘...');

    const output = {
        version: '1.0',
        createdAt: new Date().toISOString(),
        totalChunks: allChunks.length,
        documents: allChunks.map((chunk, i) => ({
            id: chunk.id,
            content: chunk.content,
            metadata: chunk.metadata,
            embedding: embeddingVectors[i]
        }))
    };

    await fs.writeFile(OUTPUT_FILE, JSON.stringify(output));
    const fileSizeKB = (JSON.stringify(output).length / 1024).toFixed(1);

    // 5. í•´ì‹œ ì €ì¥
    await saveHashes(currentHashes);

    console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    console.log('âœ… ë²¡í„° DB ë¹Œë“œ ì™„ë£Œ!');
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    console.log(`   ğŸ“Š ì´ ì²­í¬: ${allChunks.length}ê°œ`);
    console.log(`   ğŸ’¾ íŒŒì¼ í¬ê¸°: ${fileSizeKB} KB`);
    console.log(`   ğŸ“ ì €ì¥ ìœ„ì¹˜: ${OUTPUT_FILE}`);
    console.log('');
}

// ì‹¤í–‰
buildVectorDB().catch(error => {
    console.error('âŒ ë¹Œë“œ ì‹¤íŒ¨:', error);
    process.exit(1);
});
